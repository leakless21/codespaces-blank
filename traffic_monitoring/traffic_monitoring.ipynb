{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79595449",
   "metadata": {},
   "source": [
    "# Traffic Monitoring System\n",
    "\n",
    "This notebook allows you to run the Traffic Monitoring System on platforms like Google Colab, Kaggle, or any Jupyter environment. The system can:\n",
    "\n",
    "- **Detect Vehicles in Real-Time**: Spot cars, trucks, and other vehicles in video feeds\n",
    "- **Read License Plates**: Automatically identify and record license plate numbers\n",
    "- **Track Moving Vehicles**: Follow each vehicle as it moves through the video\n",
    "- **Count Traffic**: Count vehicles as they cross a line you define on the screen\n",
    "- **Store Results**: Save all detection data for later analysis\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Video Ingestion**: Gets frames from your video source\n",
    "2. **Detection**: Uses AI to find vehicles and license plates\n",
    "3. **Tracking**: Keeps track of each vehicle as it moves\n",
    "4. **Counting**: Counts vehicles when they cross your defined line\n",
    "5. **OCR**: Reads license plate text when detected\n",
    "6. **Storage**: Saves results to a database\n",
    "7. **Main Application**: Coordinates all the components and shows results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd70a5",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's set up the environment and install all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d80679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'kaggle' in sys.modules\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install opencv-python-headless numpy ultralytics onnxruntime boxmot easyocr paho-mqtt SQLite3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025920d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if we're in Colab or Kaggle\n",
    "if IN_COLAB or IN_KAGGLE or not any('traffic_monitoring' in s for s in !ls):\n",
    "    !git clone https://github.com/yourusername/traffic_monitoring.git\n",
    "    %cd traffic_monitoring\n",
    "else:\n",
    "    print(\"Repository already exists or we're running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa2cc2",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Models\n",
    "\n",
    "Now, let's download and prepare the AI models needed for vehicle detection and license plate recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Check if models already exist\n",
    "vehicle_model_path = Path('./models/yolo11s.onnx')\n",
    "plate_model_path = Path('./models/plate_v8n.onnx')\n",
    "\n",
    "if not vehicle_model_path.exists() or not plate_model_path.exists():\n",
    "    print(\"Downloading models...\")\n",
    "    # Download vehicle detection model\n",
    "    !python -c \"from ultralytics import YOLO; YOLO('yolo11s.pt')\"\n",
    "    # Move the downloaded model to our models directory\n",
    "    !mv yolo11s.pt models/ 2>/dev/null || echo \"Model may already be in place\"\n",
    "    \n",
    "    # Download plate detection model (this is a simplified example - adjust as needed)\n",
    "    !wget -q https://github.com/yourusername/traffic_monitoring/releases/download/v1.0/plate_v8n.pt -O models/plate_v8n.pt\n",
    "    \n",
    "    # Convert models to ONNX format for better performance\n",
    "    !python utils/model_converter.py --model models/yolo11s.pt --output models/yolo11s.onnx\n",
    "    !python utils/model_converter.py --model models/plate_v8n.pt --output models/plate_v8n.onnx\n",
    "else:\n",
    "    print(\"Models already exist, skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943d8fc",
   "metadata": {},
   "source": [
    "## 3. Upload a Video or Use Sample Data\n",
    "\n",
    "Now you can either upload your own video or use one of the sample videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# For Colab: provide an upload widget\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Please upload a video file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        video_path = os.path.join('data', filename)\n",
    "        with open(video_path, 'wb') as f:\n",
    "            f.write(uploaded[filename])\n",
    "        print(f\"Uploaded {filename} to {video_path}\")\n",
    "elif IN_KAGGLE:\n",
    "    # For Kaggle: list available input files\n",
    "    print(\"Available input files:\")\n",
    "    !ls ../input\n",
    "    \n",
    "    # Symlink input directory to make files accessible\n",
    "    !ln -s ../input input\n",
    "else:\n",
    "    print(\"Available sample videos:\")\n",
    "    !ls data/*.mp4 data/*.MOV data/*.avi 2>/dev/null || echo \"No sample videos found in data/ directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9643b",
   "metadata": {},
   "source": [
    "## 4. Configure the System\n",
    "\n",
    "Let's set up the configuration for the Traffic Monitoring System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified config for Jupyter environment\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Make sure config directory exists\n",
    "os.makedirs('config/settings', exist_ok=True)\n",
    "\n",
    "# Create a basic configuration\n",
    "config = {\n",
    "    'video': {\n",
    "        'source': 'data/sample.mp4',  # Will be overridden below\n",
    "        'frame_skip': 1,\n",
    "        'process_resolution': [640, 480],\n",
    "        'output_fps': 30.0\n",
    "    },\n",
    "    'detection': {\n",
    "        'vehicle_model': 'models/yolo11s.onnx',\n",
    "        'plate_model': 'models/plate_v8n.onnx',\n",
    "        'vehicle_model_version': 'yolo11',\n",
    "        'plate_model_version': 'yolov8',\n",
    "        'confidence': 0.25,\n",
    "        'iou_threshold': 0.45\n",
    "    },\n",
    "    'tracking': {\n",
    "        'tracker_type': 'bytetrack',\n",
    "        'confidence': 0.3\n",
    "    },\n",
    "    'counting': {\n",
    "        'use_relative_coordinates': True,\n",
    "        'line': {\n",
    "            'start': [0.25, 0.6],\n",
    "            'end': [0.75, 0.6]\n",
    "        },\n",
    "        'raw_coordinates': {\n",
    "            'start': [320, 360],\n",
    "            'end': [960, 360]\n",
    "        }\n",
    "    },\n",
    "    'ocr': {\n",
    "        'languages': ['en'],\n",
    "        'use_gpu': False\n",
    "    },\n",
    "    'hardware': {\n",
    "        'use_gpu': False,\n",
    "        'provider': 'auto',\n",
    "        'precision': 'fp32'\n",
    "    },\n",
    "    'storage': {\n",
    "        'database_path': 'data/traffic_data.db',\n",
    "        'save_images': False\n",
    "    },\n",
    "    'mqtt': {\n",
    "        'enabled': False,\n",
    "        'broker': 'localhost',\n",
    "        'port': 1883,\n",
    "        'topic_prefix': 'traffic'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write config to file\n",
    "with open('config/settings/config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Configuration created successfully\")\n",
    "\n",
    "# Function to update video source\n",
    "def set_video_source(source_path):\n",
    "    # Load current config\n",
    "    with open('config/settings/config.yaml', 'r') as f:\n",
    "        current_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Update video source\n",
    "    current_config['video']['source'] = source_path\n",
    "    \n",
    "    # Write updated config\n",
    "    with open('config/settings/config.yaml', 'w') as f:\n",
    "        yaml.dump(current_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Video source updated to: {source_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53584bed",
   "metadata": {},
   "source": [
    "## 5. Select Video Source\n",
    "\n",
    "Choose which video to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available videos\n",
    "import glob\n",
    "\n",
    "# Search for videos in different locations\n",
    "all_videos = []\n",
    "for ext in ['*.mp4', '*.avi', '*.MOV', '*.mkv']:\n",
    "    # Look in data directory\n",
    "    all_videos.extend(glob.glob(os.path.join('data', ext)))\n",
    "    # For Kaggle, look in input directory if exists\n",
    "    if IN_KAGGLE and os.path.exists('input'):\n",
    "        all_videos.extend(glob.glob(os.path.join('input', ext)))\n",
    "        all_videos.extend(glob.glob(os.path.join('input', '*', ext)))\n",
    "\n",
    "print(\"Found videos:\")\n",
    "for i, video in enumerate(all_videos):\n",
    "    print(f\"{i+1}. {video}\")\n",
    "\n",
    "# Allow selection\n",
    "if all_videos:\n",
    "    selection = input(f\"Enter the number of the video to use (1-{len(all_videos)}): \")\n",
    "    try:\n",
    "        idx = int(selection) - 1\n",
    "        if 0 <= idx < len(all_videos):\n",
    "            selected_video = all_videos[idx]\n",
    "            set_video_source(selected_video)\n",
    "        else:\n",
    "            print(f\"Invalid selection. Using first video: {all_videos[0]}\")\n",
    "            set_video_source(all_videos[0])\n",
    "    except ValueError:\n",
    "        print(f\"Invalid input. Using first video: {all_videos[0]}\")\n",
    "        set_video_source(all_videos[0])\n",
    "else:\n",
    "    print(\"No videos found. Please upload a video first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c62cc",
   "metadata": {},
   "source": [
    "## 6. Adjust Configuration (Optional)\n",
    "\n",
    "You can adjust various settings to customize how the system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76420f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update config parameters\n",
    "def update_config_params(**kwargs):\n",
    "    # Load current config\n",
    "    with open('config/settings/config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Update parameters\n",
    "    for key, value in kwargs.items():\n",
    "        if '.' in key:\n",
    "            # Handle nested parameters like 'video.frame_skip'\n",
    "            parts = key.split('.')\n",
    "            current = config\n",
    "            for part in parts[:-1]:\n",
    "                current = current.setdefault(part, {})\n",
    "            current[parts[-1]] = value\n",
    "        else:\n",
    "            config[key] = value\n",
    "    \n",
    "    # Write updated config\n",
    "    with open('config/settings/config.yaml', 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Configuration updated with: {kwargs}\")\n",
    "\n",
    "# Example: Adjust common parameters\n",
    "frame_skip = 1  # Process every frame (2 would process every other frame)\n",
    "detection_confidence = 0.25  # Lower = more detections but more false positives\n",
    "use_gpu = False  # Set to True if your system has GPU support\n",
    "count_line_position = 0.6  # Vertical position (0-1) of counting line\n",
    "\n",
    "update_config_params(\n",
    "    **{\n",
    "        'video.frame_skip': frame_skip,\n",
    "        'detection.confidence': detection_confidence,\n",
    "        'hardware.use_gpu': use_gpu,\n",
    "        'counting.line.start': [0.25, count_line_position],\n",
    "        'counting.line.end': [0.75, count_line_position]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920dc78",
   "metadata": {},
   "source": [
    "## 7. Run Traffic Monitoring System\n",
    "\n",
    "Now let's run the traffic monitoring system to process the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified version of the main application for Jupyter environment\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, HTML, display\n",
    "from base64 import b64encode\n",
    "from pathlib import Path\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import services - these imports will work when running inside the traffic_monitoring directory\n",
    "from services.video_ingestion.service import VideoIngestionService\n",
    "from services.detection.service import DetectionService\n",
    "from services.tracking.service import TrackingService\n",
    "from services.counting.service import CountingService\n",
    "from services.ocr.service import OCRService\n",
    "from services.storage.service import StorageService\n",
    "from config import config\n",
    "\n",
    "class JupyterTrafficMonitor:\n",
    "    \"\"\"Traffic monitoring application adapted for Jupyter environment\"\"\"\n",
    "    \n",
    "    def __init__(self, video_source=None, record_output=True, output_path=None):\n",
    "        self.video_source = video_source or config.VIDEO_SOURCE\n",
    "        self.record_output = record_output\n",
    "        self.output_path = output_path\n",
    "        self.running = False\n",
    "        self.frames = []\n",
    "        # Initialize services\n",
    "        print(\"Initializing services...\")\n",
    "        self.video_service = VideoIngestionService(source=self.video_source)\n",
    "        self.detection_service = DetectionService()\n",
    "        self.tracking_service = TrackingService()\n",
    "        self.counting_service = CountingService()\n",
    "        self.ocr_service = OCRService()\n",
    "        self.storage_service = StorageService()\n",
    "        \n",
    "        # Video writer for recording output\n",
    "        self.video_writer = None\n",
    "        self.video_writer_lock = threading.Lock()\n",
    "        self.video_queue = queue.Queue(maxsize=30)\n",
    "        self.video_writer_thread = None\n",
    "        self.video_writer_running = False\n",
    "        \n",
    "        print(\"Services initialized\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start processing\"\"\"\n",
    "        if self.running:\n",
    "            print(\"Already running\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting traffic monitoring...\")\n",
    "        \n",
    "        # Start services\n",
    "        self.storage_service.start()\n",
    "        self.video_service.start()\n",
    "        \n",
    "        # Initialize video writer if recording\n",
    "        if self.record_output:\n",
    "            self._init_video_writer()\n",
    "            if self.video_writer is not None:\n",
    "                self.video_writer_running = True\n",
    "                self.video_writer_thread = threading.Thread(target=self._video_writer_thread)\n",
    "                self.video_writer_thread.daemon = True\n",
    "                self.video_writer_thread.start()\n",
    "        \n",
    "        self.running = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ded844",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _init_video_writer(self):\n",
    "        \"\"\"Initialize video writer\"\"\"\n",
    "        # Get first frame to determine dimensions\n",
    "        frame_data = self.video_service.get_frame()\n",
    "        if frame_data is None:\n",
    "            print(\"Failed to get frame for initializing video writer\")\n",
    "            return\n",
    "        \n",
    "        frame = frame_data['frame']\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Create output path\n",
    "        if self.output_path:\n",
    "            output_path = self.output_path\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "        else:\n",
    "            output_dir = Path('data/recordings')\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = str(output_dir / f\"traffic_monitoring_{timestamp}.mp4\")\n",
    "        \n",
    "        # Ensure MP4 extension\n",
    "        if not output_path.lower().endswith('.mp4'):\n",
    "            output_path = output_path.rsplit('.', 1)[0] + '.mp4'\n",
    "        \n",
    "        print(f\"Recording output to {output_path}\")\n",
    "        \n",
    "        # Initialize writer with H264 codec\n",
    "        try:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'H264')\n",
    "            self.video_writer = cv2.VideoWriter(\n",
    "                output_path, fourcc, config.OUTPUT_FPS, (width, height))\n",
    "            \n",
    "            if not self.video_writer.isOpened():\n",
    "                # Try MP4V as fallback\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "                self.video_writer = cv2.VideoWriter(\n",
    "                    output_path, fourcc, config.OUTPUT_FPS, (width, height))\n",
    "                \n",
    "                if not self.video_writer.isOpened():\n",
    "                    # Try AVI as last resort\n",
    "                    output_path = output_path.rsplit('.', 1)[0] + '.avi'\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "                    self.video_writer = cv2.VideoWriter(\n",
    "                        output_path, fourcc, config.OUTPUT_FPS, (width, height))\n",
    "                    \n",
    "                    if not self.video_writer.isOpened():\n",
    "                        print(\"Failed to initialize video writer\")\n",
    "                        self.video_writer = None\n",
    "                        self.record_output = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing video writer: {e}\")\n",
    "            self.video_writer = None\n",
    "            self.record_output = False\n",
    "        \n",
    "        # Return frame to pipeline\n",
    "        self.video_service.rewind_one_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _video_writer_thread(self):\n",
    "        \"\"\"Thread for writing video frames\"\"\"\n",
    "        while self.video_writer_running:\n",
    "            try:\n",
    "                frame = self.video_queue.get(timeout=0.5)\n",
    "                if frame is None:\n",
    "                    break\n",
    "                \n",
    "                with self.video_writer_lock:\n",
    "                    if self.video_writer is not None:\n",
    "                        self.video_writer.write(frame)\n",
    "                \n",
    "                self.video_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video writer thread: {e}\")\n",
    "    \n",
    "    def process_frames(self, max_frames=None, display_interval=5):\n",
    "        \"\"\"Process video frames and display results at intervals\"\"\"\n",
    "        self.frames = []\n",
    "        frame_count = 0\n",
    "        total_frames = self.video_service.get_total_frames()\n",
    "        \n",
    "        try:\n",
    "            while self.running:\n",
    "                # Get frame\n",
    "                frame_data = self.video_service.get_frame()\n",
    "                if frame_data is None:\n",
    "                    print(\"Finished processing all frames\")\n",
    "                    break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "                # Process frame\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Detection\n",
    "                detection_results = self.detection_service.detect(frame_data)\n",
    "                \n",
    "                # Tracking\n",
    "                tracking_results = self.tracking_service.update(frame_data, detection_results)\n",
    "                \n",
    "                # Counting\n",
    "                counting_results = self.counting_service.update(frame_data, tracking_results)\n",
    "                \n",
    "                # OCR\n",
    "                ocr_results = self.ocr_service.read_plates(frame_data, detection_results, tracking_results)\n",
    "                \n",
    "                # Calculate FPS\n",
    "                processing_time = time.time() - start_time\n",
    "                fps = 1.0 / processing_time if processing_time > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a661946",
   "metadata": {},
   "outputs": [],
   "source": [
    "                # Prepare visualization\n",
    "                vis_frame = self._prepare_visualization(\n",
    "                    frame_data, detection_results, tracking_results, \n",
    "                    counting_results, ocr_results, fps\n",
    "                )\n",
    "                \n",
    "                # Store frame for display\n",
    "                if frame_count % display_interval == 0 or frame_count == 1:\n",
    "                    self.frames.append(vis_frame)\n",
    "                    \n",
    "                    # Display progress\n",
    "                    if total_frames > 0:\n",
    "                        progress = (frame_count / total_frames) * 100\n",
    "                        print(f\"Progress: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "                    else:\n",
    "                        print(f\"Processed {frame_count} frames\")\n",
    "                    \n",
    "                    # Show the latest frame\n",
    "                    self._display_frame(vis_frame)\n",
    "                \n",
    "                # Record output if enabled\n",
    "                if self.record_output and self.video_writer is not None and self.video_writer_running:\n",
    "                    try:\n",
    "                        if not self.video_queue.full():\n",
    "                            self.video_queue.put(vis_frame, block=False)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error queuing video frame: {e}\")\n",
    "                \n",
    "                # Check if max frames reached\n",
    "                if max_frames and frame_count >= max_frames:\n",
    "                    print(f\"Reached maximum frames: {max_frames}\")\n",
    "                    break\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Processing interrupted\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frames: {e}\")\n",
    "        finally:\n",
    "            if total_frames > 0:\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "            self.stop()\n",
    "        \n",
    "        return self.frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _prepare_visualization(self, frame_data, detection_results, tracking_results, \n",
    "                             counting_results, ocr_results, fps):\n",
    "        \"\"\"Create visualization frame\"\"\"\n",
    "        # Use original frame for visualization if available to show full resolution\n",
    "        if 'original_frame' in frame_data:\n",
    "            frame = frame_data['original_frame'].copy()\n",
    "            processed_frame = frame_data['frame'].copy()\n",
    "            \n",
    "            # Calculate scaling factors\n",
    "            scale_x = frame.shape[1] / processed_frame.shape[1]\n",
    "            scale_y = frame.shape[0] / processed_frame.shape[0]\n",
    "            \n",
    "            # Function to scale coordinates\n",
    "            def scale_coords(coords):\n",
    "                if isinstance(coords, tuple) or isinstance(coords, list):\n",
    "                    if len(coords) == 2:  # Single point (x,y)\n",
    "                        return (int(coords[0] * scale_x), int(coords[1] * scale_y))\n",
    "                    elif len(coords) == 4:  # Box (x1,y1,x2,y2)\n",
    "                        return (int(coords[0] * scale_x), int(coords[1] * scale_y), \n",
    "                                int(coords[2] * scale_x), int(coords[3] * scale_y))\n",
    "                return coords\n",
    "            \n",
    "            # Scale the counting line\n",
    "            line = counting_results['counting_line']\n",
    "            line = [scale_coords(line[0]), scale_coords(line[1])]\n",
    "        else:\n",
    "            # Fall back to processed frame if original is not available\n",
    "            frame = frame_data['frame'].copy()\n",
    "            line = counting_results['counting_line']\n",
    "            \n",
    "            def scale_coords(coords):\n",
    "                return coords  # No scaling needed for processed frame\n",
    "        \n",
    "        # Draw counting line\n",
    "        cv2.line(frame, tuple(line[0]), tuple(line[1]), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw tracks\n",
    "        for track in tracking_results['tracks']:\n",
    "            track_id = track['track_id']\n",
    "            box = scale_coords(track['box'])\n",
    "            x1, y1, x2, y2 = box\n",
    "            \n",
    "            # Color: orange for normal tracks, green for counted tracks\n",
    "            color = (0, 165, 255)  # Orange for uncounted tracks\n",
    "            if track_id in self.counting_service.counted_tracks:\n",
    "                color = (0, 255, 0)  # Green for counted tracks\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            \n",
    "            # Draw track ID with better visibility\n",
    "            text = f\"ID:{track_id}\"\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            cv2.rectangle(frame, \n",
    "                         (int(x1), int(y1) - text_size[1] - 5),\n",
    "                         (int(x1) + text_size[0] + 5, int(y1)), \n",
    "                         (0, 0, 0), -1)\n",
    "            cv2.putText(frame, text, (int(x1) + 2, int(y1) - 5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Draw path with scaled coordinates\n",
    "            path = track['path']\n",
    "            if len(path) >= 2:\n",
    "                for i in range(1, len(path)):\n",
    "                    p1 = scale_coords(path[i-1])\n",
    "                    p2 = scale_coords(path[i])\n",
    "                    cv2.line(frame, p1, p2, color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a143871",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Draw license plates\n",
    "        for plate in ocr_results['plates']:\n",
    "            box = scale_coords(plate['box'])\n",
    "            x1, y1, x2, y2 = box\n",
    "            text = plate['text']\n",
    "            \n",
    "            # Draw plate box\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            \n",
    "            # Draw text background\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            cv2.rectangle(frame, \n",
    "                         (int(x1), int(y1) - text_size[1] - 10),\n",
    "                         (int(x1) + text_size[0] + 10, int(y1)), \n",
    "                         (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(frame, text, (int(x1) + 5, int(y1) - 5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Add timestamp\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(frame_data['timestamp']))\n",
    "        cv2.putText(frame, timestamp, (20, frame.shape[0] - 20),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw counts as overlay\n",
    "        counts = counting_results['counts']\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        # Draw semi-transparent background\n",
    "        cv2.rectangle(overlay, (10, 10), (300, 70), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n",
    "        \n",
    "        # Draw total count text - larger and more prominent\n",
    "        cv2.putText(frame, f\"TOTAL COUNT: {counts['total']}\", (20, 50),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw FPS\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (frame.shape[1] - 150, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e89b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _display_frame(self, frame):\n",
    "        \"\"\"Display a frame in the Jupyter notebook\"\"\"\n",
    "        # Convert to JPEG\n",
    "        _, jpeg = cv2.imencode('.jpg', frame)\n",
    "        # Convert to base64\n",
    "        jpeg_b64 = b64encode(jpeg).decode('utf-8')\n",
    "        \n",
    "        # Display\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(f'''\n",
    "            <div style=\"text-align:center;\">\n",
    "                <img src=\"data:image/jpeg;base64,{jpeg_b64}\" style=\"max-width:100%;\" />\n",
    "            </div>\n",
    "        '''))\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop processing\"\"\"\n",
    "        if not self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = False\n",
    "        print(\"Stopping traffic monitoring...\")\n",
    "        \n",
    "        # Stop video writer\n",
    "        if self.video_writer_running:\n",
    "            self.video_writer_running = False\n",
    "            self.video_queue.put(None)  # Signal to exit\n",
    "            if self.video_writer_thread:\n",
    "                self.video_writer_thread.join(timeout=2.0)\n",
    "        \n",
    "        if self.video_writer is not None:\n",
    "            with self.video_writer_lock:\n",
    "                self.video_writer.release()\n",
    "                self.video_writer = None\n",
    "        \n",
    "        # Stop services\n",
    "        self.video_service.stop()\n",
    "        self.storage_service.stop()\n",
    "        \n",
    "        print(\"Traffic monitoring stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_video_from_frames(self, output_path=None, fps=30):\n",
    "        \"\"\"Create a video from processed frames\"\"\"\n",
    "        if not self.frames:\n",
    "            print(\"No frames to create video from\")\n",
    "            return\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_dir = Path('data/recordings')\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = str(output_dir / f\"traffic_monitoring_summary_{timestamp}.mp4\")\n",
    "        \n",
    "        # Get dimensions from first frame\n",
    "        height, width = self.frames[0].shape[:2]\n",
    "        \n",
    "        # Create writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Write frames\n",
    "        for frame in self.frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Created summary video with {len(self.frames)} frames at {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the traffic monitoring system\n",
    "monitor = JupyterTrafficMonitor().start()\n",
    "\n",
    "# Process frames (set max_frames to limit processing, or None for all frames)\n",
    "# display_interval controls how often to show progress (higher = faster processing, fewer updates)\n",
    "frames = monitor.process_frames(max_frames=None, display_interval=10)\n",
    "\n",
    "# Create a summary video with key frames\n",
    "summary_video = monitor.create_video_from_frames(fps=5)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e708a",
   "metadata": {},
   "source": [
    "## 8. Download Results\n",
    "\n",
    "If you're using Colab or Kaggle, you can download the processed video and any data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Function to download a file\n",
    "    def download_file(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            files.download(file_path)\n",
    "            print(f\"Downloading {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Find output videos\n",
    "    output_videos = glob.glob('data/recordings/*.mp4')\n",
    "    for video in output_videos:\n",
    "        print(f\"Found output video: {video}\")\n",
    "    \n",
    "    # Download the latest video\n",
    "    if output_videos:\n",
    "        latest_video = max(output_videos, key=os.path.getmtime)\n",
    "        download_file(latest_video)\n",
    "    \n",
    "    # Download the database\n",
    "    download_file('data/traffic_data.db')\n",
    "else:\n",
    "    print(\"Files saved to:\")\n",
    "    !ls -lh data/recordings/ | grep mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e1169",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "You've successfully run the Traffic Monitoring System in a Jupyter environment! Here's what we accomplished:\n",
    "\n",
    "1. Set up the necessary environment\n",
    "2. Downloaded and prepared AI models\n",
    "3. Configured the system\n",
    "4. Processed a video to detect, track, count vehicles and read license plates\n",
    "5. Recorded and saved the results\n",
    "\n",
    "This notebook makes it easy to use the Traffic Monitoring System on platforms like Google Colab and Kaggle without needing to install anything locally.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try the system with different videos\n",
    "- Adjust configuration parameters for better performance\n",
    "- Explore the database for detailed analysis\n",
    "- Check out the full documentation in the [GitHub repository](https://github.com/yourusername/traffic_monitoring)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
